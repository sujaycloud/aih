{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MIMIC-IV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icu = pd.read_csv('../mimic_iv_data/icustays.csv.gz', compression='gzip')\n",
    "df_patients = pd.read_csv('../mimic_iv_data/patients.csv.gz', compression='gzip')\n",
    "df_chart_events = pd.read_csv('../mimic_iv_data/chartevents.csv.gz', compression='gzip')\n",
    "df_d_items = pd.read_csv('../mimic_iv_data/d_items.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_events = pd.read_csv('../mimic_iv_data/labevents.csv.gz', compression='gzip')\n",
    "df_d_labitems = pd.read_csv('../mimic_iv_data/d_labitems.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup and Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get age, gender from the patients table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients.info()\n",
    "df_patients_f = df_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subject_id, gender, anchor_age, dod\n",
    "selected_columns = ['subject_id', 'gender', 'anchor_age', 'dod']\n",
    "df_patients_f = df_patients.loc[:, selected_columns]\n",
    "df_patients_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace dod with disceased, NaN with 0, and not NaN with 1\n",
    "df_patients_f['dod'] = df_patients_f['dod'].notna().astype(int)\n",
    "# Rename the columns\n",
    "df_patients_f.rename(columns={'dod': 'deceased'}, inplace=True)\n",
    "# Change gender M to 0 and F to 1\n",
    "df_patients_f['gender'] = df_patients_f['gender'].replace({'M': 0, 'F': 1})\n",
    "df_patients_f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the age histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of anchor_age\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_patients_f['anchor_age'], bins=30, kde=True)\n",
    "plt.title('Distribution of Anchor Age')\n",
    "plt.xlabel('Anchor Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only patients with anchor_age > 0\n",
    "df_patients_f = df_patients_f[df_patients_f['anchor_age'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find number of patients who are deceased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of patients who are deceased\n",
    "deceased_count = df_patients_f['deceased'].value_counts()\n",
    "print(deceased_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the itemids for the chart events (Heart Rate, Blood Pressure, SpO2, Temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d_items.info()\n",
    "df_d_items.head()\n",
    "# Find item for Heart Rate, Blood Pressure, SpO2, Temperature\n",
    "df_d_items[df_d_items['label'].str.contains('Heart Rate', na=False)].head()\n",
    "df_d_items[df_d_items['label'].str.contains('Blood Pressure', na=False)].head()\n",
    "df_d_items[df_d_items['label'].str.contains('SpO2', na=False)].head()\n",
    "df_d_items[df_d_items['label'].str.contains('Temperature', na=False)].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get only the critical vitals from the chart events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_events_list = [220045, 220050, 220051, 226253, 223761, 220210, \n",
    "                    220179, 220180, 220181, 223762, 220277, 223835, 224700]\n",
    "for itemid in chart_events_list:\n",
    "    print(f'{itemid}: {df_d_items[df_d_items[\"itemid\"] == itemid][\"label\"].values[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter the chart events for only the desired events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_chart_events_f = df_chart_events[df_chart_events['itemid'].isin(chart_events_list)]\n",
    "# Remove duplicate items in itemid\n",
    "df_chart_events_f.drop_duplicates(subset=['subject_id', 'itemid'], inplace=True)\n",
    "df_chart_events_f.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the relevant columns\n",
    "selected_columns = ['subject_id', 'hadm_id', 'stay_id', 'itemid', 'value']\n",
    "df_chart_events_f = df_chart_events_f.loc[:, selected_columns]\n",
    "df_chart_events_f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pivot and create new columns for the chart event list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot on itemid\n",
    "df_chart_events_pivot = df_chart_events_f.pivot(index='subject_id', columns='itemid', values='value').reset_index()\n",
    "# Rename the columns\n",
    "chart_events_list_str = [str(itemid) for itemid in chart_events_list]\n",
    "df_chart_events_pivot.columns = ['subject_id'] + chart_events_list_str\n",
    "df_chart_events_pivot.dropna(axis=0, inplace=True)\n",
    "df_chart_events_pivot.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the critical lab events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_events_list = [50813, 50882, 50912, 50809, 51221]\n",
    "for itemid in lab_events_list:\n",
    "    print(f'{itemid}: {df_d_labitems[df_d_labitems[\"itemid\"] == itemid][\"label\"].values[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter the lab events table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_events_f = df_lab_events[df_lab_events['itemid'].isin(lab_events_list)]\n",
    "# Remove duplicate items in itemid\n",
    "df_lab_events_f.drop_duplicates(subset=['subject_id', 'itemid'], inplace=True)\n",
    "df_lab_events_f.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the relevant columns\n",
    "selected_columns = ['subject_id', 'itemid', 'value']\n",
    "df_lab_events_f = df_lab_events_f.loc[:, selected_columns]\n",
    "df_lab_events_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot on itemid\n",
    "df_lab_events_pivot = df_lab_events_f.pivot(index='subject_id', columns='itemid', values='value').reset_index()\n",
    "lab_events_list_str = [str(itemid) for itemid in lab_events_list]\n",
    "df_lab_events_pivot.columns = ['subject_id'] + lab_events_list_str\n",
    "df_lab_events_pivot.dropna(axis=0, inplace=True)\n",
    "df_lab_events_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merge the chart, patients and lab events table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with patients\n",
    "df_merged = df_chart_events_pivot.merge(df_patients_f, how='inner', on='subject_id')\n",
    "df_merged = df_merged.merge(df_lab_events_pivot, how='inner', on='subject_id')\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the label(target) and the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the merged dataframe\n",
    "df_merged.to_csv('merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the merged dataframe\n",
    "df_merged = pd.read_csv('merged_data.csv')\n",
    "target = df_merged['deceased']\n",
    "X = df_merged.drop(columns=['subject_id', 'deceased'])\n",
    "y = target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Regression models for comparison\n",
    "models = [SGDRegressor(random_state = 0),\n",
    "          GradientBoostingRegressor(random_state = 0),\n",
    "          LinearRegression(),\n",
    "          KNeighborsRegressor(),\n",
    "          RandomForestRegressor(random_state = 0),]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    # Instantiate and fit Regressor Model\n",
    "    reg_model = model\n",
    "    reg_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions with model\n",
    "    y_test_preds = reg_model.predict(X_test)\n",
    "\n",
    "    # Grab model name and store results associated with model\n",
    "    name = str(model).split(\"(\")[0]\n",
    "\n",
    "    results[name] = r2_score(y_test, y_test_preds)\n",
    "    print('{} done.'.format(name))\n",
    "    print(\"R^2 Score:\", r2_score(y_test, y_test_preds))\n",
    "    print(\"Mean Squared Error:\", mean_squared_error(y_test, y_test_preds))\n",
    "    #print(classification_report(y_test, (y_test_preds >= 0.5).astype(int)))\n",
    "\n",
    "    # Confusion matrix\n",
    "    y_test_preds = (y_test_preds >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_test, y_test_preds)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 score results\n",
    "fig, ax = plt.subplots()\n",
    "ind = range(len(results))\n",
    "ax.barh(ind, list(results.values()), align='center',\n",
    "        color = '#55a868', alpha=0.8)\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(results.keys())\n",
    "ax.set_xlabel('R-squared score')\n",
    "ax.tick_params(left=False, top=False, right=False)\n",
    "ax.set_title('Comparison of Regression Models')\n",
    "#fig.savefig('images/compare_models.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingRegressor will be used as the deceased prediction model\n",
    "reg_model = GradientBoostingRegressor(random_state=0)\n",
    "reg_model.fit(X_train, y_train)\n",
    "y_test_preds = reg_model.predict(X_test)\n",
    "r2_not_refined = r2_score(y_test, y_test_preds)\n",
    "print(\"R2 score is: {:2f}\".format(r2_not_refined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train 80% and test 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    target,\n",
    "                                                    test_size = .20,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "#tuned_parameters = [{'n_estimators': [100, 200, 300],\n",
    "#                     'max_depth' : [2, 3, 4],\n",
    "#                     'loss': ['ls', 'lad', 'huber']}]\n",
    "tuned_parameters = [{'n_estimators': [200, 300],\n",
    "                     'max_depth' : [3, 4],\n",
    "                     'loss': ['squared_error', 'absolute_error']}]\n",
    "\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "reg_model = GradientBoostingRegressor()\n",
    "grid = GridSearchCV(reg_model, tuned_parameters)\n",
    "grid.fit(X_train, y_train)\n",
    "reg_model_optimized = grid.best_estimator_\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg_model = GradientBoostingRegressor(n_estimators = 200, max_depth=4, random_state=0)\n",
    "#reg_model.fit(X_train, y_train)\n",
    "y_test_preds = reg_model_optimized.predict(X_test)\n",
    "r2_optimized = r2_score(y_test, y_test_preds)\n",
    "print(\"Optimized R2 score is: {:2f}\".format(r2_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the new confusion matrix after optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = (y_test_preds >= 0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, y_test_preds)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Model refinement improved R2 score by {:.4f}'.format(r2_optimized-r2_not_refined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert the data frame to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df_merged to a tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X = df_merged.drop(columns=['subject_id', 'deceased'])\n",
    "y = df_merged['deceased']\n",
    "\n",
    "# Convert all of the columns to float32\n",
    "X = X.astype(np.float32)\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32).to(device='mps')\n",
    "print(X_tensor.shape)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32).to(device='mps')\n",
    "print(y_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_tensor = (X_tensor - X_tensor.mean(dim=0)) / X_tensor.std(dim=0)\n",
    "y_tensor = (y_tensor - y_tensor.mean(dim=0)) / y_tensor.std(dim=0)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the date into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training (80%) and testing sets (20%)\n",
    "n_samples = X_tensor.shape[0]\n",
    "n_val = int(n_samples * 0.2)\n",
    "n_train = n_samples - n_val\n",
    "train_data = X_tensor[:n_train].to(device='mps')\n",
    "val_data = X_tensor[n_train:].to(device='mps')\n",
    "train_labels = y_tensor[:n_train].to(device='mps')\n",
    "val_labels = y_tensor[n_train:].to(device='mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a binary classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classification model\n",
    "class BinaryClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassificationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "input_size = X_tensor.shape[1]\n",
    "model = BinaryClassificationModel(input_size).to(device='mps')\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_data)\n",
    "    loss = criterion(outputs.squeeze(), train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "\n",
    "    val_outputs = model(val_data)\n",
    "    val_loss = criterion(val_outputs.squeeze(), val_labels)\n",
    "\n",
    "    # Convert predictions to binary (0 or 1) for classification metrics\n",
    "    val_preds = (val_outputs.squeeze() >= 0.5).float()\n",
    "    \n",
    "    # Ensure the predictions are integers for compatibility with classification metrics\n",
    "    val_preds = val_preds.int()\n",
    "    val_labels = val_labels.int()\n",
    "    \n",
    "    accuracy = accuracy_score(val_labels.int().cpu(), val_preds.int().cpu())\n",
    "    print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
