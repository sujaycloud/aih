{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMIC NLP Assignment\n",
    "### Read the MIMIC-III files ([DIAGNOSES_ICD](https://physionet.org/content/mimiciii/1.4/DIAGNOSES_ICD.csv.gz), [NOTEEVENTS](https://physionet.org/content/mimiciii/1.4/NOTEEVENTS.csv.gz), [D_ICD_DIAGNOSES](https://physionet.org/content/mimiciii/1.4/D_ICD_DIAGNOSES.csv.gz))\n",
    "\n",
    "Note: After download, make sure to unzip the files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read the downloaded files into the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "diag_df = pd.read_csv('DIAGNOSES_ICD.csv')\n",
    "notes_df = pd.read_csv('NOTEEVENTS.csv')\n",
    "d_diag_df = pd.read_csv('D_ICD_DIAGNOSES.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find the most common diagnosed disease and convert to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_df.info()\n",
    "disease = diag_df['ICD9_CODE'].value_counts().nlargest(1)\n",
    "# Create a list of the top 5 diseases\n",
    "disease_list = disease.index.tolist()\n",
    "print(disease_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a new CSV (disease_notes.csv) file with the notes belonging to the disease list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_df = diag_df[diag_df['ICD9_CODE'].isin(disease_list)]\n",
    "\n",
    "# Merge with notes_df to get the text of the notes\n",
    "merged_df = pd.merge(disease_df, notes_df, on='SUBJECT_ID', how='inner').dropna()\n",
    "merged_df.to_csv('disease_notes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install spacy and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U pip setuptools wheel\n",
    "! pip install -U spacy\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocessing function for the clinical notes to reduce the noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess1(x):\n",
    "    y=re.sub('\\\\[(.*?)\\\\]','',x) #remove de-identified brackets\n",
    "    y=re.sub('[0-9]+\\.','',y) #remove 1.2. since the segmenter segments based on this\n",
    "    y=re.sub('dr\\.','doctor',y)\n",
    "    y=re.sub('m\\.d\\.','md',y)\n",
    "    y=re.sub('admission date:','',y)\n",
    "    y=re.sub('discharge date:','',y)\n",
    "    y=re.sub('--|__|==','',y)\n",
    "    \n",
    "    # remove, digits, spaces\n",
    "    y = y.translate(str.maketrans(\"\", \"\", string.digits))\n",
    "    y = \" \".join(y.split())\n",
    "    return y\n",
    "\n",
    "def preprocessing(df_notes): \n",
    "    df_notes['TEXT']=df_notes['TEXT'].fillna(' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.replace('\\n',' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.replace('\\r',' ')\n",
    "    df_notes['TEXT']=df_notes['TEXT'].apply(str.strip)\n",
    "    df_notes['TEXT']=df_notes['TEXT'].str.lower()\n",
    "\n",
    "    df_notes['TEXT']=df_notes['TEXT'].apply(lambda x: preprocess1(x))\n",
    "    \n",
    "    return df_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the model and read the medical notes from the disease_notes.csv file into the untrained model, after running it through the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "merged_notes_df = pd.read_csv('disease_notes.csv')\n",
    "merged_notes_df = preprocessing(merged_notes_df)\n",
    "notes = merged_notes_df['TEXT'].tolist()\n",
    "print(len(notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print each note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "for i in range(len(notes)):\n",
    "  doc.append(nlp(notes[i]))\n",
    "  print(doc[-1])\n",
    "  print('*************************************************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(doc)):\n",
    "    for token in doc[i]:\n",
    "        print(token.text, token.pos_)\n",
    "    print('**************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token\n",
    "token_without_punct = []\n",
    "for i in range(len(doc)):\n",
    "  token_without_punct.append([token.orth_ for token in doc[i] if not token.is_punct | token.is_space])\n",
    "  print(token_without_punct[-1])\n",
    "  print('*******************************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "for i in range(len(doc)):\n",
    "    for ent in doc[i].ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    print('**************************************************')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entity Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity Visualizer\n",
    "from spacy import displacy\n",
    "for i in range(len(doc)):\n",
    "  displacy.render(doc[i], style=\"ent\", jupyter=True)\n",
    "  print('*********************************************************************************************************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence identifier\n",
    "for i in range(len(doc)):\n",
    "  for ix, sent in enumerate(doc[i].sents, 1):\n",
    "    print(\"Sentence number {}:{}\".format(ix, sent))\n",
    "  print('*******************************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependence tree\n",
    "for i in range(len(doc)):\n",
    "  sentence_spans = list(doc[i].sents)\n",
    "  displacy.render(sentence_spans, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciSpacy Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install scispacy and the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install Scispacy\n",
    "! pip install -U spacy\n",
    "! pip install scispacy\n",
    "\n",
    "! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz\n",
    "! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_craft_md-0.5.4.tar.gz\n",
    "! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_jnlpba_md-0.5.4.tar.gz\n",
    "! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz\n",
    "! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz\n",
    "! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the untrained model, read the clinical notes and preprocess them from disease_notes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import en_core_sci_md\n",
    "\n",
    "nlp = en_core_sci_md.load()\n",
    "merged_notes_df = pd.read_csv('disease_notes.csv')\n",
    "\n",
    "merged_notes_df = preprocessing(merged_notes_df)\n",
    "\n",
    "notes = merged_notes_df['TEXT'].tolist()\n",
    "print(len(notes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "for i in range(len(notes)):\n",
    "  doc.append(nlp(notes[i]))\n",
    "  print(doc[-1])\n",
    "  print('*************************************************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(doc)):\n",
    "    for token in doc[i]:\n",
    "        print(token.text, token.pos_)\n",
    "    print('**************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token\n",
    "token_without_punct = []\n",
    "for i in range(len(doc)):\n",
    "  token_without_punct.append([token.orth_ for token in doc[i] if not token.is_punct | token.is_space])\n",
    "  print(token_without_punct[-1])\n",
    "  print('*******************************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "for i in range(len(notes)):\n",
    "  doc.append(nlp(notes[i]))\n",
    "  for ent in doc[-1].ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "  print(\"****************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entity Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity Visualizer\n",
    "from spacy import displacy\n",
    "for i in range(len(doc)):\n",
    "  displacy.render(doc[i], style=\"ent\", jupyter=True)\n",
    "  print(\"************************************************************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_ner_bc5cdr_md\n",
    "nlp = en_ner_bc5cdr_md.load()\n",
    "doc = []\n",
    "for i in range(len(notes)):\n",
    "  doc.append(nlp(notes[i]))\n",
    "  displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "  print(\"*******************************************************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install gensim and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim\n",
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the CORE untrained model and read in the disease_notes.csv file after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "merged_notes_df = pd.read_csv('disease_notes.csv')\n",
    "merged_notes_df = preprocessing(merged_notes_df)\n",
    "notes = merged_notes_df['TEXT'].tolist()\n",
    "print(len(notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build corpus of all the entities extracted from the notes using spaCy model.\n",
    "# The corpus is an array of arrays or list of lists where each of the nested lists corresponds to a note.\n",
    "corpus=[]\n",
    "for row in range(0, len(notes)):\n",
    "  str_tokens=[]\n",
    "  tokens= nlp(notes[row]).ents\n",
    "  for i in range(0, len(tokens)):\n",
    "    str_tokens.append(tokens[i].text)\n",
    "  corpus.append(list(str_tokens))\n",
    "\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model1 = Word2Vec(corpus, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.wv['fentanyl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.wv.most_similar('fentanyl', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE plots  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model,words, preTrained=False):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in words:\n",
    "      if preTrained:\n",
    "          tokens.append(model[word])\n",
    "      else:\n",
    "          tokens.append(model.wv[word])\n",
    "      labels.append(word)\n",
    "\n",
    "    tokens = np.array(tokens)\n",
    "    tsne_model = TSNE(perplexity=30, early_exaggeration=12, n_components=2, init='pca', n_iter=1000, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE plots for the **untrained** Model\n",
    "Notice how noisy the scatter plot is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = model1.wv.key_to_index.keys()\n",
    "new_v = np.array(list(vocabs))\n",
    "tsne_plot(model1,new_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE plot for a **trained** model\n",
    "Notice the word embeddings are better clustered together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word2vec embeddings\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info()  # show info about available models/datasets\n",
    "pretrained_model= api.load(\"glove-wiki-gigaword-50\")  # download the model and return as object ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.most_similar(\"heart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_in_pretrained_model = []\n",
    "for word in vocabs:\n",
    "  if word in pretrained_model:\n",
    "    corpus_in_pretrained_model.append(word)\n",
    "  else:\n",
    "    print(word) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(pretrained_model,corpus_in_pretrained_model,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedSpacy Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install medspacy and import the relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install medspacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import spacy\n",
    "import medspacy\n",
    "\n",
    "from medspacy.ner import TargetMatcher, TargetRule\n",
    "from medspacy.visualization import visualize_ent, visualize_dep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable={\"ner\"})\n",
    "nlp = medspacy.load(nlp=nlp)\n",
    "\n",
    "merged_notes_df = pd.read_csv('disease_notes.csv')\n",
    "merged_notes_df = preprocessing(merged_notes_df)\n",
    "notes = merged_notes_df['TEXT']\n",
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add target rules into the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.ner import TargetRule\n",
    "\n",
    "nlp.get_pipe('medspacy_target_matcher').add([TargetRule('Hypertension', 'CONDITION'), \n",
    "                                             TargetRule('heart failure', 'CONDITION'), \n",
    "                                             TargetRule('pna', 'CONDITION'), \n",
    "                                             TargetRule('diabetes', 'CONDITION'), \n",
    "                                             TargetRule('stroke', 'CONDITION'), \n",
    "                                             TargetRule('hx of stroke', 'CONDITION'), \n",
    "                                             TargetRule('hx of diabetes', 'CONDITION')])\n",
    "# doc = nlp('Patient has hx of stroke. Mother diagnosed with diabetes. No evidence of pna.')\n",
    "for i in range(len(notes)):\n",
    "    doc = nlp(notes[i])\n",
    "    print('*************************************************************************************************************')\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    print('*************************************************************************************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(notes)):               \n",
    "    medspacy.visualization.visualize_ent(nlp(notes[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
